"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1460],{1771:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"cool-stuff/primer","title":"Certifai Algorithm Primer","description":"This page is an informational primer on how the algorithms work in the Certifai application.\\n","source":"@site/docs/cool-stuff/primer.md","sourceDirName":"cool-stuff","slug":"/cool-stuff/primer","permalink":"/documentarian/docs/cool-stuff/primer","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Certifai Algorithm Primer","linkTitle":"Certifai Algorithm Primer","description":"This page is an informational primer on how the algorithms work in the Certifai application.\\n"},"sidebar":"templateSidebar","previous":{"title":"Jira Story Writing Checklist","permalink":"/documentarian/docs/cool-stuff/jira-story-checklist"}}');var a=t(4848),s=t(8453);const o={title:"Certifai Algorithm Primer",linkTitle:"Certifai Algorithm Primer",description:"This page is an informational primer on how the algorithms work in the Certifai application.\n"},r=void 0,l={},c=[{value:"Overview",id:"overview",level:2},{value:"Certifai for Binary Classifiers",id:"certifai-for-binary-classifiers",level:2},{value:"Counterfactuals",id:"counterfactuals",level:2},{value:"Risk Evaluation Using Counterfactuals",id:"risk-evaluation-using-counterfactuals",level:2},{value:"Extensions to Regression Settings",id:"extensions-to-regression-settings",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"This page is an informational primer on how the algorithms work in the Certifai application."}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"Certifai is a risk assessment tool that repeatedly probes a predictive model M in terms of its input-output behavior and provides an\nevaluation of model risk along 3 dimensions:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Robustness (R)"}),"\n",(0,a.jsx)(n.li,{children:"Explainability (E)"}),"\n",(0,a.jsx)(n.li,{children:"Fairness/Bias (F)."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"It treats the model M as a black\nbox - thus it only needs to know the model\u2019s response or output corresponding to a given instance or input. This means that any kind\nof predictive model can be assessed, including rule-based systems, statistical approaches, and neural networks. Certifai can be used to\nassess regression as well as classification models (binary and multi-class) models. However, its application in a binary classification setting\nis the easiest to understand and is briefly described below."}),"\n",(0,a.jsx)(n.h2,{id:"certifai-for-binary-classifiers",children:"Certifai for Binary Classifiers"}),"\n",(0,a.jsx)(n.p,{children:"Without loss of generality, we refer to the two classes as the\npositive and negative class respectively, with the positive class\nbeing a more desired outcome. As a running example, we\nconsider a loan approval case, in which each instance is a loan\napplication, the positive class is \u201capproved\u201d and the negative\nclass is \u201cdenied\u201d. Any classifier will partition the input space into\nregions that are assigned the same class label, separated by\ndecision boundaries."}),"\n",(0,a.jsx)(n.h2,{id:"counterfactuals",children:"Counterfactuals"}),"\n",(0,a.jsx)(n.p,{children:"Certifai is based on the notion of a counterfactual (CF), as used\nin the recent fairness literature, e.g., Wachter et al., 2017 [1] and\nGoogle\u2019s What-if Tool [2]. Given an instance, a CF is a data-point\nfor which the classifier returns a different class label. The most\nrelevant CFs are those that are as close as possible to the given\ninstance. Then, the difference between the feature vectors\nrepresenting the probe and its corresponding CF represents the\n(minimal) change that needs to be made to the probe in order\nto flip the outcome. Given an input instance (the \u201cprobe\u201d) and\na black-box classifier, Certifai generates a series of (synthetic)\ninstances based on a genetic algorithm, to efficiently query the\nmodel and estimate the location of the CF that is nearest to the\nprobe. We use the L1 distance metric and induce a sparsity prior\nas well so that a CF tends to not differ from the probe in many\nattributes. These properties make the CF more explainable and\nactionable, as explained later."}),"\n",(0,a.jsx)(n.p,{children:"Note that the CF depends on the given probe as well as the\ngiven model M. We guarantee that the model will return the\nopposite class label for the CF. Moreover, while in general it is not\npossible to guarantee (given limited compute time) that the CF\nis the closest possible, the CFs that Certifai finds are in general\nsubstantially closer than those from alternative approaches,\nincluding the one taken by Google [2], which restricts the CF to\nbe an actual data point in the evaluation dataset. Moreover, the\nuser can specify domain constraints to ensure that the\nCF is realistic. For example, in a medical application, the user\ncan specify that demographics (gender, ethnicity) of an instance\ncannot change, but only behaviors and treatments (exercise,\nmedications) can change in the CF. Additionally, Certifai\nhas the option of returning multiple CFs, representing\nqualitatively disparate options, all of which lead to a different\noutcome."}),"\n",(0,a.jsx)(n.h2,{id:"risk-evaluation-using-counterfactuals",children:"Risk Evaluation Using Counterfactuals"}),"\n",(0,a.jsx)(n.p,{children:"Robustness (R), Explainability (E) and Fairness/Bias (F) are all joint\nproperties of the model being assessed, as well as the application\nthe model has been designed for. A proper evaluation thus\nrequires an evaluation dataset, D, that is representative of the\napplication of interest. For each instance in this dataset, Certifai\ndetermines a suitable CF. All the CFs thus determined then\ncontribute to the model risk evaluation as follows:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robustness"}),": The distance of a CF to the corresponding\nprobe point, averaged over D, is a measure of robustness or sensitivity, since it indicates the average amount of\n(adversarial) perturbation needed to flip the outcome.\nHigher scores indicate less sensitive or more robust\nmodels. Scores can also be normalized to a range from 0 to\n100 using a proprietary non-linear but monotonic function\nof the CF distance after normalization w.r.t intra-class data\nspreads."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Explainability"}),": A CF can be expressed in terms of the\nnumber of attributes that need to change in order to flip\nthe outcome. Fewer attributes changing means that the\nexplanation on what it will minimally take to change the\noutcome (for example to convert an application from\ndenied to accepted status) is more succinct and hence\nmore explainable. Each CF gets a explainability score\nthat is a monotonically decreasing function of the number\nof attributes involved in the change vector. The mean\nexplainability score is a measure of the explainability of\nthe entire model, normalized to a range from 0 to 100. In\naddition to the model-level explainability, if CF individual\nexplanations are needed for specific probes or instances,\nsuch instances can be collated and specified in an\n\u201cexplanation dataset\u201d."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fairness/Bias"}),": For fairness studies, one has to first define\none or more categorical variables, called the grouping\nfeature(s) (aka protected attribute(s)), which is used to\npartition the instances into subgroups. For example, the\ngroupings can be based on gender, ethnicity, a combination\nof gender and age, etc. Then, fairness is evaluated by\ncomparing the outcomes or burden imposed by the model\nacross the different subgroups. For a two-class problem,\nwe consider the burden to be zero for a given instance if\nit receives the desired outcome (positive class). Otherwise\nthe burden is indicated by the difficulty of recourse required\nto flip the outcome from negative to positive, as measured\nby the corresponding CF distance. Finally, the average\nburden across the different subgroups is compared using\nthe gini index, a popular measure of inequality. A score of 100 means that each group has the same average burden,\nwhile a score of 0 means that one subgroup has all the\nburden while other groups have no burden at all."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"It is well known that there are many indicators of fairness\nand different notions of justice, such as distributional justice\nand procedural justice. Machine learning based fairness\nassessments (e.g. demographic parity, equalized odds) typically\nfocus on distributive justice and consider only binary outcomes.\nOur approach is more nuanced, since it considers not only the\noutcome but also how difficult it is to attain a more preferred\noutcome."}),"\n",(0,a.jsx)(n.h2,{id:"extensions-to-regression-settings",children:"Extensions to Regression Settings"}),"\n",(0,a.jsx)(n.p,{children:"For regression, an alternate outcome can be defined in terms of\nrelative or absolute thresholds of the predicted value, depending\non the application. For example, if the current model produces\na function f(x), then two derived functions, f(x) + \u03bb\u03c3 and f(x) - \u03bb\u03c3\ncan serve as upper and lower boundaries respectively, where\n\u03bb is a user defined value, and \u03c3 is the standard deviation of the\noutcome. Alternatively, if one is predicting a credit score, then\na fixed value, say 650 for FICO, can be specified, such at any\nscore above this value is deemed as desirable. Thus we split\nthe outcome in two or three regimes, \u201ccurrent\u201d, \u201chigher\u201d and\n\u201clower\u201d, depending on one or two boundaries being specified,\nconverting it into a binary or 3-class problem."}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Sandra Wachter, Brent Mittelstadt, and Chris Russell.\nCounterfactual explanations without opening the black box:\nautomated decisions and the GDPR. Harvard Journal of Law\n& Technology, 31(2):2018, 2017"}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://pair-code.github.io/what-if-tool/",children:"https://pair-code.github.io/what-if-tool/"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://plato.stanford.edu/entries/causation-counterfactual/",children:"https://plato.stanford.edu/entries/causation-counterfactual/"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);