<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-tutorial-extras/ceritfai-basics/factors/fairness" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Fairness | Susan Michalski - Documentation Specialist</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://smichalski17-dev.github.io/documentarian/img/typwriter.jpg"><meta data-rh="true" name="twitter:image" content="https://smichalski17-dev.github.io/documentarian/img/typwriter.jpg"><meta data-rh="true" property="og:url" content="https://smichalski17-dev.github.io/documentarian/docs/tutorial-extras/ceritfai-basics/factors/fairness"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Fairness | Susan Michalski - Documentation Specialist"><meta data-rh="true" name="description" content="This page provides details about the Fairness trust factor in Cortex Certifai.
"><meta data-rh="true" property="og:description" content="This page provides details about the Fairness trust factor in Cortex Certifai.
"><link data-rh="true" rel="canonical" href="https://smichalski17-dev.github.io/documentarian/docs/tutorial-extras/ceritfai-basics/factors/fairness"><link data-rh="true" rel="alternate" href="https://smichalski17-dev.github.io/documentarian/docs/tutorial-extras/ceritfai-basics/factors/fairness" hreflang="en"><link data-rh="true" rel="alternate" href="https://smichalski17-dev.github.io/documentarian/docs/tutorial-extras/ceritfai-basics/factors/fairness" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Documents","item":"https://smichalski17-dev.github.io/documentarian/docs/category/documents"},{"@type":"ListItem","position":2,"name":"Fairness","item":"https://smichalski17-dev.github.io/documentarian/docs/tutorial-extras/ceritfai-basics/factors/fairness"}]}</script><link rel="alternate" type="application/rss+xml" href="/documentarian/blog/rss.xml" title="Susan Michalski - Documentation Specialist RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/documentarian/blog/atom.xml" title="Susan Michalski - Documentation Specialist Atom Feed"><link rel="stylesheet" href="/documentarian/assets/css/styles.6d484712.css">
<script src="/documentarian/assets/js/runtime~main.9d40e250.js" defer="defer"></script>
<script src="/documentarian/assets/js/main.ce0b64b7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/documentarian/img/avatar.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/documentarian/"><div class="navbar__logo"><img src="/documentarian/img/avatar.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/documentarian/img/avatar.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/documentarian/docs/intro">Documentation</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/documentarian/docs/intro"><span title="Documentaion Samples" class="linkLabel_WmDU">Documentaion Samples</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/documentarian/docs/category/about-me"><span title="About Me" class="categoryLinkLabel_W154">About Me</span></a><button aria-label="Expand sidebar category &#x27;About Me&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--active" href="/documentarian/docs/category/documents"><span title="Documents" class="categoryLinkLabel_W154">Documents</span></a><button aria-label="Collapse sidebar category &#x27;Documents&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/atx"><span title="ceritfai-basics" class="categoryLinkLabel_W154">ceritfai-basics</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/atx"><span title="factors" class="categoryLinkLabel_W154">factors</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/atx"><span title="AI Trust Index" class="linkLabel_WmDU">AI Trust Index</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/data-statistics"><span title="Data Statistics and Drift Monitoring" class="linkLabel_WmDU">Data Statistics and Drift Monitoring</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/explainability"><span title="Explainability and Explanations" class="linkLabel_WmDU">Explainability and Explanations</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/fairness"><span title="Fairness" class="linkLabel_WmDU">Fairness</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/performance-metric"><span title="Performance Metric" class="linkLabel_WmDU">Performance Metric</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/robustness"><span title="Robustness" class="linkLabel_WmDU">Robustness</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/ceritfai-basics/interpret-scan-results"><span title="Interpret Scan Results" class="linkLabel_WmDU">Interpret Scan Results</span></a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/kubernetes-installation"><span title="Kubernetes Installation" class="linkLabel_WmDU">Kubernetes Installation</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/documentarian/docs/tutorial-extras/monitoring-dashboard"><span title="Monitoring Dashboard" class="linkLabel_WmDU">Monitoring Dashboard</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/documentarian/docs/category/templates"><span title="Templates" class="categoryLinkLabel_W154">Templates</span></a><button aria-label="Expand sidebar category &#x27;Templates&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist" href="/documentarian/docs/category/cool-stuff"><span title="Cool Stuff" class="categoryLinkLabel_W154">Cool Stuff</span></a><button aria-label="Expand sidebar category &#x27;Cool Stuff&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/documentarian/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/documentarian/docs/category/documents"><span>Documents</span></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">ceritfai-basics</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">factors</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Fairness</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Fairness</h1></header><p>This page provides details about the Fairness trust factor in Cortex Certifai. Certifai supports multiple measures of Fairness which are described on this page, including:</p>
<ul>
<li class="">Burden</li>
<li class="">Demographic Parity</li>
<li class="">Predictive Parity (Sufficiency)</li>
<li class="">Equalized Odds</li>
<li class="">Equal Opportunity</li>
</ul>
<p><strong>Demographic Parity</strong> and <strong>Burden</strong> have the advantage of not requiring ground truth (labeled data), and they are not subject to bias that may have occurred in the labeling (e.g. historical bias).</p>
<p><strong>Equal Opportunity</strong>, <strong>Equalized Odds</strong>, and <strong>Predictive Parity</strong> are useful when bias in ground truth is not a concern.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-fairness">What is Fairness?<a href="#what-is-fairness" class="hash-link" aria-label="Direct link to What is Fairness?" title="Direct link to What is Fairness?" translate="no">​</a></h2>
<p>Fairness is a measure of the disparity between the change required to alter the outcome in categorical groups defined by the fairness grouping feature.</p>
<p>Fairness is a particular concern in AI systems because bias exhibited by predictive models can render models untrustworthy and unfair to one or more target groups.</p>
<p>For example, different models can exhibit any number of biases towards features like gender, age, or educational level.</p>
<p>Features targeted for fairness evaluation may be numeric (e.g. age) or non-numeric (e.g. marital status).</p>
<p>Fairness metrics are based on the prediction results depicted in the &#x27;confusion matrix&#x27; (e.g. demographic parity) to determine if unfairness has occurred - regardless of whether the outcome is favorable or unfavorable and whether or not it was predicted correctly. These metrics require ground truth labels.</p>
<p>However, traditional fairness metrics fail to outline how one could overcome this inequality or analyze results in the absence of ground truth labels. To address these shortcomings, we devise a fairness metric in Certifai called Burden, which is based on the closest counterfactuals of data points. This allows a user to gain a perspective of how difficult it may be to obtain a favorable outcome since the counterfactual exists on the decision boundary.</p>
<p><strong>Example:</strong>
In the use case of binary classification models that are predicting if a loan applicant will be granted or denied a loan, Certifai users might want to decide which model shows a higher level of fairness between male, female, and self-assigning applicants. In this case, the user would run a scan choosing &quot;gender&quot; as one of the fairness grouping features. The report generated by Scan Manager would assign results as &quot;favorable&quot; for applicants granted a loan and &quot;unfavorable&quot; for applicants denied a loan.</p>
<p>Fairness reports may be selected as one of the output types in Certifai Scan Manager.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="fairness-metrics">Fairness Metrics<a href="#fairness-metrics" class="hash-link" aria-label="Direct link to Fairness Metrics" title="Direct link to Fairness Metrics" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-equalized-odds">What is Equalized Odds?<a href="#what-is-equalized-odds" class="hash-link" aria-label="Direct link to What is Equalized Odds?" title="Direct link to What is Equalized Odds?" translate="no">​</a></h3>
<p>The Equalized Odds fairness metric is based on the &#x27;confusion matrix&#x27;. Thus, the metric depends on the predicted outcome of the model and the ground truth labels.</p>
<p>Equalized odds ensures that the probability of a positive prediction is the same for both favorable and unfavorable truth labels across all groups of a feature. In other words, the model&#x27;s true positive rates and false positive rates are the same across groups.</p>
<p>Equal Odds is useful when positive predictions are important and false positives are a concern (e.g. granting a loan).</p>
<p>For example: Equalized Odds states that the favorable outcome (being granted a loan) occurs at roughly the same percent of the time for all protected groups (males and females) (AND correspondingly the unfavorable outcome occurs at roughly the same percent of the time).</p>
<p><strong>Example: Equalized Odds based on income and using payment history</strong></p>
<p><img decoding="async" loading="lazy" alt="Equalized Odds depiction" src="/documentarian/assets/images/fairness-equalized-odds-9afb04af6d9fa222fd02c58e91a53bd9.png" width="1286" height="882" class="img_ev3q"></p>
<p><em><a href="https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2" target="_blank" rel="noopener noreferrer" class="">Image taken from &quot;How to define fairness to detect and prevent discriminatory outcomes in Machine Learning&quot; by Valeria Cortez (Sept. 23, 2019)</a></em></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-equal-opportunity">What is Equal Opportunity?<a href="#what-is-equal-opportunity" class="hash-link" aria-label="Direct link to What is Equal Opportunity?" title="Direct link to What is Equal Opportunity?" translate="no">​</a></h3>
<p>Equal Opportunity is also based on the &#x27;confusion matrix&#x27;. Thus, the metric depends on the predicted outcome of the model and the ground truth labels.</p>
<p>Equal Opportunity fairness metric verifies that odds of obtaining a favorable result are equal for a single protected group across a specified attribute&#x27;s values.</p>
<p>Equal opportunity is useful when positive predictions are important, as long as false positives are not a concern (e.g. detecting fraudulent transactions).</p>
<p>Equal opportunity ensures that if 80% of qualified males get the favorable outcome, a similar percentage of females do. Where &quot;qualified&quot; means &quot;favorable ground truth&quot;.</p>
<p><strong>Example B: Equal Opportunity based on income and using payment history</strong></p>
<p><img decoding="async" loading="lazy" alt="Equal Opportunity depiction" src="/documentarian/assets/images/fairness-equal-opp-3c179551a443ae09dd0f7b032198e654.png" width="1278" height="888" class="img_ev3q"></p>
<p><em><a href="https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2" target="_blank" rel="noopener noreferrer" class="">Image taken from &quot;How to define fairness to detect and prevent discriminatory outcomes in Machine Learning&quot; by Valeria Cortez (Sept. 23, 2019)</a></em></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-demographic-parity">What is Demographic Parity?<a href="#what-is-demographic-parity" class="hash-link" aria-label="Direct link to What is Demographic Parity?" title="Direct link to What is Demographic Parity?" translate="no">​</a></h3>
<p>Demographic Parity ensures that the proportion of individuals in each protected group (e.g. male/female) of a protected feature (e.g. gender) receives a positive outcome at equal rates. (e.g. both men and women receive auto loans 40% of the time across all zip codes).</p>
<p>Demographic parity is often used where regulations require the minimization of disparate impact.</p>
<p><strong>Example: Demographic based on income and using payment history</strong></p>
<p><img decoding="async" loading="lazy" alt="Demographic disparity depiction" src="/documentarian/assets/images/fairness-demographic-parity-590290771581099d62445905df248763.png" width="1422" height="1070" class="img_ev3q"></p>
<p><em><a href="https://towardsdatascience.com/how-to-define-fairness-to-detect-and-prevent-discriminatory-outcomes-in-machine-learning-ef23fd408ef2" target="_blank" rel="noopener noreferrer" class="">Image taken from &quot;How to define fairness to detect and prevent discriminatory outcomes in Machine Learning&quot; by Valeria Cortez (Sept. 23, 2019)</a></em></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-predictive-parity-sufficiency">What is Predictive Parity (Sufficiency)?<a href="#what-is-predictive-parity-sufficiency" class="hash-link" aria-label="Direct link to What is Predictive Parity (Sufficiency)?" title="Direct link to What is Predictive Parity (Sufficiency)?" translate="no">​</a></h3>
<p>Predictive Parity (or Sufficiency) is based on the &#x27;confusion matrix&#x27;. Thus, the metric depends on the predicted outcome of the model and the ground truth labels.</p>
<p>Predictive Parity verifies that the precision rates for protected groups are (e.g. male/female) of a protected feature (e.g. gender) are roughly equivalent.</p>
<p>For example Predictive Parity ensures that the model predicting the rate of loans being granted is accurate across zip codes.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-burden">What is Burden?<a href="#what-is-burden" class="hash-link" aria-label="Direct link to What is Burden?" title="Direct link to What is Burden?" translate="no">​</a></h3>
<p>Burden is a group-based metric from the perspective of the &quot;unfavorable&quot; outcome. burdens allows a user to gain a perspective of how difficult it may be to obtain a favorable outcome.</p>
<p>Burden can provide a more nuanced metric reflecting the distribution of predictions. It is also the only metric currently supported for regression use cases.</p>
<p>Burden represents a weighted proportion of individual groups in a protected feature who experience a negative outcome. This weight is determined by the average distance of a group to the outcome boundary between a favorable and unfavorable outcome (derived by counterfactuals). We can interpret this distance as the &quot;burden&quot; an individual must overcome to receive a favorable outcome.</p>
<p>Fairness Burden scores are derived by using counterfactuals to find the degree to which the values of an input data point must change to obtain a favorable prediction from the model.</p>
<p>The Burden is the average amount of change required for members of feature groups to achieve a favorable result (e.g. loan granted).</p>
<p>A lower number indicates that less change is required for that group to have a favorable outcome.</p>
<p>If the burden for one group is very high and the burden for another group is low, the model may be unfair to the group with the higher burden.</p>
<p>Ideally, the objective is to have burdens as close to one another as possible, which indicates the model is treating members within the selected groups fairly.</p>
<p>It is important to note that burden is not comparable across features. They are only designed to compare the groups within a single feature. (e.g you cannot compare fairness between age and marital status.)</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="burden-based-fairness-concepts">Burden-based Fairness Concepts<a href="#burden-based-fairness-concepts" class="hash-link" aria-label="Direct link to Burden-based Fairness Concepts" title="Direct link to Burden-based Fairness Concepts" translate="no">​</a></h4>
<h5 class="anchor anchorTargetStickyNavbar_Vzrq" id="favorability-values-for-classification">Favorability Values for Classification<a href="#favorability-values-for-classification" class="hash-link" aria-label="Direct link to Favorability Values for Classification" title="Direct link to Favorability Values for Classification" translate="no">​</a></h5>
<p>Favorability options give users alternative ways to define what outcomes are considered favorable.</p>
<ul>
<li class=""><strong>None</strong>: Changes are neither favorable nor unfavorable, therefore a change to any other outcome class is a counterfactual. (NOTE: Fairness cannot be calculated with favorability of NONE.)</li>
<li class=""><strong>Explicit/Partitioned</strong>: Identifies a change to any class in the other partition is counterfactual; a favorable change is from the unfavorable partition to the favorable one</li>
<li class=""><strong>Ordered</strong>: Outcome classes have an implicit ordering from most to least favorable. Any change in outcome class may be sufficient to be considered a counterfactual, but it is worth noting that the change is relative to the original value.</li>
</ul>
<h5 class="anchor anchorTargetStickyNavbar_Vzrq" id="defining-a-favorable-outcome-for-regression">Defining a favorable outcome for Regression<a href="#defining-a-favorable-outcome-for-regression" class="hash-link" aria-label="Direct link to Defining a favorable outcome for Regression" title="Direct link to Defining a favorable outcome for Regression" translate="no">​</a></h5>
<p>Unlike Classification tasks where a favorable outcome corresponds with one or more outcome classes, defining a favorable outcome for a Regression task can depend on the distribution being predicted.</p>
<p>Certifai allows users to define a predicted outcome as being favorable for Regression tasks relative to starting point or via a fixed threshold.</p>
<p>For example, in a use case where Regression models are predicting the total insurance claim amount for an individual, let us assume that having receiving a higher claim amount is a more favorable result. An increase from $500 to $501 might not seem favorable when compared to a potential increase from $500 to $10,000. Alternatively, we might consider any claim amount above $5000 as favorable, and any claim below as unfavorable.</p>
<p><strong>Relative change</strong>:</p>
<ul>
<li class="">A significant increase or decrease from the original prediction value is considered favorable.</li>
<li class="">The amount of change that is considered significant is specified in terms of standard deviations for the predicted outcome.</li>
</ul>
<p><strong>Absolute change</strong>:</p>
<ul>
<li class="">Defines a threshold as a percentile or absolute value.</li>
<li class="">Favorable outcomes are either strictly above or below the threshold.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="fairness-calculations">Fairness calculations<a href="#fairness-calculations" class="hash-link" aria-label="Direct link to Fairness calculations" title="Direct link to Fairness calculations" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-is-fairness-calculated">How is Fairness Calculated?<a href="#how-is-fairness-calculated" class="hash-link" aria-label="Direct link to How is Fairness Calculated?" title="Direct link to How is Fairness Calculated?" translate="no">​</a></h3>
<p>Certifai aggregates the metrics for each group into a feature fairness score, where 100% is perfectly fair (no disparity). The overall fairness score is taken to be the minimum score for any fairness feature.</p>
<p>For assessment of disparate impact, it is common to assess the maximum disparity across the groups. Certifai uses maximum disparity for all of the metrics other than burden.</p>
<p>Certifai uses Gini to aggregate burden across the groups, giving an indication of the mean disparity.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-is-burden-calculated">How is Burden calculated?<a href="#how-is-burden-calculated" class="hash-link" aria-label="Direct link to How is Burden calculated?" title="Direct link to How is Burden calculated?" translate="no">​</a></h3>
<p>Here are the brief steps for calculating fairness:</p>
<ol>
<li class="">Calculate burden for each group.</li>
<li class="">Compute the Gini index for each feature group.</li>
<li class="">Convert to a percentage that is (1 - GiniIndex).</li>
</ol>
<p>If all feature group burdens are equal, you get a 100% fair system, and if only one group has non-zero burden but none of the others do, you get 0.</p>
<p><strong>Example</strong>:
In a binary classification problem where the positive class is the desired outcome:</p>
<ul>
<li class="">If the instance is classified as positive, there is 0 burden.</li>
<li class="">If the instance is classified as negative, burden is a monotonic function, f() of the distance to the decision boundary, as this distance indicates how much work will need to be done (i.e. the burden) to flip to the desired outcome.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-are-parity-and-equality-fairness-scores-calculated">How are Parity and Equality Fairness Scores Calculated?<a href="#how-are-parity-and-equality-fairness-scores-calculated" class="hash-link" aria-label="Direct link to How are Parity and Equality Fairness Scores Calculated?" title="Direct link to How are Parity and Equality Fairness Scores Calculated?" translate="no">​</a></h3>
<p>The Parity and Equality Fairness Feature score is based on the maximum discrepancy in the metric between fairness groupings.</p>
<p>For example, in demographic parity if 75% of men receive a positive outcome, and only 50% of women do, then the fairness score is Calculated as follows: <code>1-((0.75-0.5)/0.75) = 66.7%</code>.</p>
<p><strong>NOTE</strong>: The denominator is the largest outcome group, regardless of whether the outcome is positive or negative. If 25% of men and 20% of women obtained a favorable outcome, then 75% or men and 80% of women had unfavorable outcomes so the score would be calculated as follows: <code>1-((0.25-0.2)/0.8) = 93.75%</code></p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-the-importance-of-fairness">What Is the Importance of Fairness?<a href="#what-is-the-importance-of-fairness" class="hash-link" aria-label="Direct link to What Is the Importance of Fairness?" title="Direct link to What Is the Importance of Fairness?" translate="no">​</a></h2>
<p>Based on fairness scores the following actions might be taken by the various Certifai users:</p>
<ul>
<li class="">
<p><strong>Data scientists</strong> might attempt to retrain any of the models with datasets that contain more equal distributions for age and status and compare the results again.</p>
</li>
<li class="">
<p><strong>Business decision-makers</strong> might select the fairest model for production deployment of their loan application</p>
</li>
<li class="">
<p><strong>Compliance officers</strong> might reject the use of models with overall scores or feature category differences that don’t meet a certain threshold for fairness since that is a critical aspect of trusted AI.</p>
</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="how-is-fairness-displayed-in-certifai">How is Fairness Displayed in Certifai?<a href="#how-is-fairness-displayed-in-certifai" class="hash-link" aria-label="Direct link to How is Fairness Displayed in Certifai?" title="Direct link to How is Fairness Displayed in Certifai?" translate="no">​</a></h2>
<p>The Fairness by Group visualizations are displayed in the Console:</p>
<ul>
<li class="">An overall view that shows a the average fairness score for each model.</li>
<li class="">A model view that shows a histogram of the fairness scores for each grouping feature in the model.</li>
</ul>
<p>At the top of the page, the favorable outcome for the scan is identified, (for example &quot;Loan Granted&quot; for the Banking<!-- -->:Loan<!-- --> Approval sample use case).</p>
<p>Each of the Fairness features that were defined for the scan is displayed in a separate graph. In the case of the Banking<!-- -->:Loan<!-- --> Approval use case the features are status and age.</p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/explainability"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Explainability and Explanations</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/documentarian/docs/tutorial-extras/ceritfai-basics/factors/performance-metric"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Performance Metric</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-fairness" class="table-of-contents__link toc-highlight">What is Fairness?</a></li><li><a href="#fairness-metrics" class="table-of-contents__link toc-highlight">Fairness Metrics</a><ul><li><a href="#what-is-equalized-odds" class="table-of-contents__link toc-highlight">What is Equalized Odds?</a></li><li><a href="#what-is-equal-opportunity" class="table-of-contents__link toc-highlight">What is Equal Opportunity?</a></li><li><a href="#what-is-demographic-parity" class="table-of-contents__link toc-highlight">What is Demographic Parity?</a></li><li><a href="#what-is-predictive-parity-sufficiency" class="table-of-contents__link toc-highlight">What is Predictive Parity (Sufficiency)?</a></li><li><a href="#what-is-burden" class="table-of-contents__link toc-highlight">What is Burden?</a></li></ul></li><li><a href="#fairness-calculations" class="table-of-contents__link toc-highlight">Fairness calculations</a><ul><li><a href="#how-is-fairness-calculated" class="table-of-contents__link toc-highlight">How is Fairness Calculated?</a></li><li><a href="#how-is-burden-calculated" class="table-of-contents__link toc-highlight">How is Burden calculated?</a></li><li><a href="#how-are-parity-and-equality-fairness-scores-calculated" class="table-of-contents__link toc-highlight">How are Parity and Equality Fairness Scores Calculated?</a></li></ul></li><li><a href="#what-is-the-importance-of-fairness" class="table-of-contents__link toc-highlight">What Is the Importance of Fairness?</a></li><li><a href="#how-is-fairness-displayed-in-certifai" class="table-of-contents__link toc-highlight">How is Fairness Displayed in Certifai?</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/suemich/" target="_blank" rel="noopener noreferrer" class="footer__link-item">My LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.susan-michalski.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">My Author Website<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>